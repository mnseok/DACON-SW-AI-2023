{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d73d24e3-5c9e-4ade-9e6e-ca6f46a2d914",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad9b681e-370a-4cfa-a452-dd2d7f0cd77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fc5462",
   "metadata": {},
   "source": [
    "## Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea57993",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_HEIGHT = 256\n",
    "IMAGE_WIDTH = 256\n",
    "IMAGE_CHANNEL = 3\n",
    "BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ff3de5-0d0e-497b-ac75-d5179a3f65d3",
   "metadata": {},
   "source": [
    "\n",
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "838e1d83-8670-407b-82f6-bf9652f58639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RLE 디코딩 함수\n",
    "def rle_decode(mask_rle, shape):\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape)\n",
    "\n",
    "# RLE 인코딩 함수\n",
    "def rle_encode(mask):\n",
    "    pixels = mask.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fae85aa",
   "metadata": {},
   "source": [
    "## Loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04f01d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "\n",
    "# def canny(img):\n",
    "#     tmp = cv2.erode(img, kernel=np.ones(shape=(3, 3), dtype=np.float32) * 2, iterations=3)\n",
    "#     tmp = cv2.Canny(tmp, 127, 255)\n",
    "#     tmp = cv2.dilate(tmp, kernel=np.ones(shape=(3, 3), dtype=np.float32) * 2, iterations=2)\n",
    "#     return tmp\n",
    "\n",
    "\n",
    "# def BIoU(prd, gnd, function=canny, return_img=False):\n",
    "\n",
    "#     prd, gnd = function(prd), function(gnd)\n",
    "\n",
    "#     tp, fp, fn, _ = miou(prd, gnd)\n",
    "\n",
    "#     if not return_img:\n",
    "#         return tp / (tp + fp + fn) if tp + fp + fn != 0 else 1.0\n",
    "#     else:\n",
    "#         return tp / (tp + fp + fn) if tp + fp + fn != 0 else 1.0, prd, gnd\n",
    "\n",
    "\n",
    "# def miou(pred, anno):\n",
    "\n",
    "#     tp = np.logical_and(pred, anno)\n",
    "#     tp = np.asarray(tp, 'float64')\n",
    "#     tp = np.sum(tp)\n",
    "\n",
    "#     fp = np.logical_and(np.logical_not(anno), pred)\n",
    "#     fp = np.asarray(fp, 'float64')\n",
    "#     fp = np.sum(fp)\n",
    "\n",
    "#     fn = np.logical_and(np.logical_not(pred), anno)\n",
    "#     fn = np.asarray(fn, 'float64')\n",
    "#     fn = np.sum(fn)\n",
    "\n",
    "#     tn = np.logical_and(np.logical_not(pred), np.logical_not(anno))\n",
    "#     tn = np.asarray(tn, 'float64')\n",
    "#     tn = np.sum(tn)\n",
    "\n",
    "#     return tp, fp, fn, tn\n",
    "\n",
    "\n",
    "# def iou_value(pred, anno):\n",
    "#     tp, fp, fn, tn = miou(pred, anno)\n",
    "#     return tp / (tp + fp + fn) if tp + fp + fn != 0 else 1.0\n",
    "\n",
    "\n",
    "# def biou_value(pred, anno):\n",
    "#     return BIoU(pred, anno, return_img=False)\n",
    "#canny에서 kernel과 iterations 조건에 맞는지 확인 부탁함\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def s1_loss(outputs,labels):\n",
    "    criterion = nn.BCELoss()\n",
    "    bceLoss = criterion(outputs, labels)\n",
    "    return bceLoss + BLoss(outputs, labels)\n",
    "\n",
    "def BLoss(outputs, labels):\n",
    "    return IoU(edge(outputs), edge(labels))\n",
    "\n",
    "def IoU(ou_edge, la_edge):\n",
    "    intersection = torch.logical_and(ou_edge, la_edge).sum()\n",
    "    union = torch.logical_or(ou_edge, la_edge).sum()\n",
    "    iou = intersection.float() / (union.float() + 1e-8)  # Adding a small epsilon to avoid division by zero\n",
    "    return 1 - iou\n",
    "\n",
    "def edge(input):\n",
    "    img = input.detach().cpu().numpy()\n",
    "    tmp = canny(img)\n",
    "    output = torch.from_numpy(tmp)\n",
    "    return output\n",
    "\n",
    "def canny(img):\n",
    "    tmp = cv2.erode(img, kernel=np.ones(shape=(3, 3), dtype=np.float32) * 2, iterations=1)\n",
    "    tmp = cv2.Canny(tmp, 127, 255)\n",
    "    tmp = cv2.dilate(tmp, kernel=np.ones(shape=(3, 3), dtype=np.float32) * 2, iterations=1)\n",
    "    return tmp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be76a29e-e9c2-411a-a569-04166f074184",
   "metadata": {},
   "source": [
    "## Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8496767-2f64-4285-bec4-c6f53a1fd9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SatelliteDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None, infer=False):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.infer = infer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data.iloc[idx, 1]\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.infer:\n",
    "            if self.transform:\n",
    "                image = self.transform(image=image)['image']\n",
    "            return image\n",
    "\n",
    "        mask_rle = self.data.iloc[idx, 2]\n",
    "        mask = rle_decode(mask_rle, (image.shape[0], image.shape[1]))\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc955893-22fd-4320-88be-7aa0d790cbd9",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b708503-2ff9-4584-9d73-40990b3572f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m transform \u001b[39m=\u001b[39m A\u001b[39m.\u001b[39mCompose(\n\u001b[1;32m      2\u001b[0m     [   \n\u001b[1;32m      3\u001b[0m         A\u001b[39m.\u001b[39mResize(\u001b[39m224\u001b[39m, \u001b[39m224\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m     ]\n\u001b[1;32m      7\u001b[0m )\n\u001b[0;32m----> 9\u001b[0m dataset \u001b[39m=\u001b[39m SatelliteDataset(csv_file\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m./train.csv\u001b[39;49m\u001b[39m'\u001b[39;49m, transform\u001b[39m=\u001b[39;49mtransform)\n\u001b[1;32m     10\u001b[0m dataloader \u001b[39m=\u001b[39m DataLoader(dataset, batch_size\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, num_workers\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m, in \u001b[0;36mSatelliteDataset.__init__\u001b[0;34m(self, csv_file, transform, infer)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, csv_file, transform\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, infer\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m----> 3\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(csv_file)\n\u001b[1;32m      4\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39m=\u001b[39m transform\n\u001b[1;32m      5\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfer \u001b[39m=\u001b[39m infer\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    579\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1662\u001b[0m     f,\n\u001b[1;32m   1663\u001b[0m     mode,\n\u001b[1;32m   1664\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1665\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1666\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1667\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1668\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1669\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1670\u001b[0m )\n\u001b[1;32m   1671\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    860\u001b[0m             handle,\n\u001b[1;32m    861\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    862\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    863\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    864\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    865\u001b[0m         )\n\u001b[1;32m    866\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './train.csv'"
     ]
    }
   ],
   "source": [
    "transform = A.Compose(\n",
    "    [   \n",
    "        A.Resize(224, 224),\n",
    "        A.Normalize(),\n",
    "        ToTensorV2()\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset = SatelliteDataset(csv_file='./train.csv', transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42501fc-b573-4893-a7c4-5e280dfdaf09",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65960bfb-803a-4c40-b713-6f647779e4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "def conv_layer(input, input_channel, output_channel, mean=0.0, std=1.0, bias=0.0, filter_size=3, name=None):\n",
    "    if name is None:\n",
    "        name = 'conv_layer'\n",
    "\n",
    "    conv = nn.Conv2d(input_channel, output_channel, kernel_size=filter_size, padding=filter_size // 2)\n",
    "    conv.weight.data.normal_(mean, std)\n",
    "    conv.bias.data.fill_(bias)\n",
    "\n",
    "    return conv(input)\n",
    "\n",
    "\n",
    "def deconv_layer(input, input_channel, output_channel, mean=0.0, std=1.0, bias=0.0, filter_size=3, stride=2,\n",
    "                 name=None, batch_size=1):\n",
    "    if name is None:\n",
    "        name = 'deconv_layer'\n",
    "\n",
    "    deconv = nn.ConvTranspose2d(input_channel, output_channel, kernel_size=filter_size, stride=stride,\n",
    "                                padding=filter_size // 2, output_padding=1)\n",
    "    deconv.weight.data.normal_(mean, std)\n",
    "    deconv.bias.data.fill_(bias)\n",
    "\n",
    "    return deconv(input)\n",
    "\n",
    "\n",
    "def batch_norm(x, n_out, decay=0.99, eps=1e-5, name=None, trainable=True):\n",
    "    if name is None:\n",
    "        name = 'norm'\n",
    "\n",
    "    norm = nn.BatchNorm2d(n_out, eps=eps, momentum=decay, affine=trainable)\n",
    "    return norm(x)\n",
    "\n",
    "\n",
    "def max_pooling(input, size=2):\n",
    "    return F.max_pool2d(input, kernel_size=size, stride=size)\n",
    "\n",
    "\n",
    "class B3SM(nn.Module):\n",
    "    def __init__(self, batch_size, num_channels=3):\n",
    "        super(B3SM, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "    def conv_resi_conv(self, input, channel1, channel2, channel3, name='crc'):\n",
    "        layer1 = self.conv(input, channel1, channel2, name='%s_crc1' % name)\n",
    "        layer2 = self.resi_block(layer1, channel2, name='%s_crc2' % name)\n",
    "        layer3 = self.conv(layer2, channel2, channel3, name='%s_crc3' % name)\n",
    "        return layer3\n",
    "\n",
    "    def resi_block(self, layer, channel, name):\n",
    "        layer1 = self.conv(layer, channel, channel, name=\"%s_resi_block01\" % name)\n",
    "        layer2 = self.conv(layer1, channel, channel, name=\"%s_resi_block02\" % name)\n",
    "        layer3 = self.conv(layer2, channel, channel, name=\"%s_resi_block03\" % name)\n",
    "        layer4 = layer + layer3\n",
    "        return F.relu(layer4)\n",
    "\n",
    "    def conv(self, input, input_channel, output_channel, name='layer'):\n",
    "        layer = conv_layer(input, input_channel, output_channel, filter_size=3, name=name)\n",
    "        layer = F.relu(layer)\n",
    "        layer = F.dropout(layer, p=0.0)\n",
    "        layer = batch_norm(layer, output_channel, name=name)\n",
    "        return layer\n",
    "\n",
    "    def deconv(self, input, input_channel, output_channel, name='layer'):\n",
    "        layer = deconv_layer(input, input_channel, output_channel, filter_size=3, stride=2, name=name)\n",
    "        layer = F.dropout(layer, p=0.0)\n",
    "        layer = batch_norm(layer, output_channel, name=name)\n",
    "        return layer\n",
    "\n",
    "    def merge(self, layer1, layer2):\n",
    "        return layer1 + layer2\n",
    "\n",
    "    def fusion2Block(self, input, channel):\n",
    "        layer011 = self.conv(input, channel, 64, name='layer1011')\n",
    "\n",
    "        layer112 = self.conv_resi_conv(layer011, 64, 64, 64, name='layer1112')\n",
    "        maxpool11 = max_pooling(layer112)\n",
    "\n",
    "        layer113 = self.conv_resi_conv(maxpool11, 64, 128, 128, name='layer1113')\n",
    "        maxpool12 = max_pooling(layer113)\n",
    "\n",
    "        layer114 = self.conv_resi_conv(maxpool12, 128, 256, 256, name='layer1114')\n",
    "        maxpool13 = max_pooling(layer114)\n",
    "\n",
    "        layer115 = self.conv_resi_conv(maxpool13, 256, 512, 512, name='layer1115')\n",
    "        maxpool14 = max_pooling(layer115)\n",
    "\n",
    "        # bridge\n",
    "        layer116 = self.conv_resi_conv(maxpool14, 512, 1024, 1024, name='layer1116')\n",
    "        layer116 = self.deconv(layer116, 1024, 512, name='upscaling005')\n",
    "\n",
    "        # deconv\n",
    "        upscaling4 = self.conv_resi_conv(layer116, 512, 512, 512, name='up04')\n",
    "        upscaling4 = self.USIM(maxpool14, upscaling4, self.batch_size)\n",
    "        upscaling4 = self.conv(upscaling4, 512, 256, name='upscaling004')\n",
    "\n",
    "        upscaling3 = self.conv_resi_conv(upscaling4, 256, 256, 256, name=\"up03\")\n",
    "        upscaling3 = self.USIM(maxpool13, upscaling3, self.batch_size)\n",
    "        upscaling3 = self.conv(upscaling3, 256, 128, name='upscaling003')\n",
    "\n",
    "        upscaling2 = self.conv_resi_conv(upscaling3, 128, 128, 128, name=\"up02\")\n",
    "        upscaling2 = self.USIM(maxpool12, upscaling2, self.batch_size)\n",
    "        upscaling2 = self.conv(upscaling2, 128, 64, name='upscaling002')\n",
    "\n",
    "        conv = conv_layer(upscaling2, 64, 2, filter_size=1, name='conv')\n",
    "        pred = torch.argmax(conv, dim=1)\n",
    "        return pred, conv\n",
    "\n",
    "    def fusionBlock(self, input, channel):\n",
    "        layer011 = self.conv(input, channel, 64, name='layer011')\n",
    "\n",
    "        layer112 = self.conv_resi_conv(layer011, 64, 64, 64, name='layer112')\n",
    "        maxpool11 = self.max_pooling(layer112)\n",
    "\n",
    "        layer113 = self.conv_resi_conv(maxpool11, 64, 128, 128, name='layer113')\n",
    "        maxpool12 = self.max_pooling(layer113)\n",
    "\n",
    "        layer114 = self.conv_resi_conv(maxpool12, 128, 256, 256, name='layer114')\n",
    "        maxpool13 = self.max_pooling(layer114)\n",
    "\n",
    "        layer115 = self.conv_resi_conv(maxpool13, 256, 512, 512, name='layer115')\n",
    "        maxpool14 = self.max_pooling(layer115)\n",
    "\n",
    "        # bridge\n",
    "        layer116 = self.conv_resi_conv(maxpool14, 512, 1024, 1024, name='layer116')\n",
    "\n",
    "        # deconv\n",
    "        upscaling4 = self.deconv(layer116, 1024, 512, name='upscaling4')\n",
    "        upscaling4 = self.merge(upscaling4, layer115)\n",
    "        upscaling4 = self.conv_resi_conv(upscaling4, 512, 512, 512, name='up4')\n",
    "\n",
    "        upscaling3 = self.deconv(upscaling4, 512, 256, name='upscaling3')\n",
    "        upscaling3 = self.merge(upscaling3, layer114)\n",
    "        upscaling3 = self.conv_resi_conv(upscaling3, 256, 256, 256, name=\"up3\")\n",
    "\n",
    "        upscaling2 = self.deconv(upscaling3, 256, 128, name=\"upscaling2\")\n",
    "        upscaling2 = self.merge(upscaling2, layer113)\n",
    "        upscaling2 = self.conv_resi_conv(upscaling2, 128, 128, 128, name=\"up2\")\n",
    "\n",
    "        upscaling1 = self.deconv(upscaling2, 128, 64, name='upscaling1')\n",
    "        upscaling1 = self.merge(upscaling1, layer112)\n",
    "        upscaling1 = self.conv_resi_conv(upscaling1, 64, 64, 64, name='up1')\n",
    "\n",
    "        conv = conv_layer(upscaling1, 64, 2, filter_size=1, name='conv_tmp')\n",
    "        pred = torch.argmax(conv, dim=1)\n",
    "        return pred, conv\n",
    "\n",
    "\n",
    "    def USIM(self, layer1, layer2, batch):\n",
    "        _, H, W, C = layer2.size()\n",
    "\n",
    "        resized1 = F.interpolate(layer1, size=(2 * H, 2 * W), mode='nearest')\n",
    "        resized2 = F.interpolate(layer2, size=(2 * H, 2 * W), mode='nearest')\n",
    "\n",
    "        def get_init_values(h, w, flag='LU'):\n",
    "            init_value = torch.zeros((2 * h, 2 * w), dtype=torch.float32)\n",
    "            if flag == 'LU':\n",
    "                H, W = 0, 0\n",
    "            elif flag == 'RU':\n",
    "                H, W = 0, 1\n",
    "            elif flag == 'LD':\n",
    "                H, W = 1, 0\n",
    "            elif flag == 'RD':\n",
    "                H, W = 1, 1\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "            for i in range(H, 2 * h, 2):\n",
    "                for j in range(W, 2 * w, 2):\n",
    "                    init_value[i, j] = 1.0\n",
    "\n",
    "            return init_value\n",
    "\n",
    "        slsh_init = get_init_values(H, W, 'RU') + get_init_values(H, W, 'LD')\n",
    "        slsh_init = slsh_init.unsqueeze(0).unsqueeze(3)\n",
    "        bslh_init = get_init_values(H, W, 'LU') + get_init_values(H, W, 'RD')\n",
    "        bslh_init = bslh_init.unsqueeze(0).unsqueeze(3)\n",
    "\n",
    "        slsh = nn.Parameter(slsh_init, requires_grad=False)\n",
    "        bslh = nn.Parameter(bslh_init, requires_grad=False)\n",
    "\n",
    "        layer = resized1 * slsh + resized2 * bslh\n",
    "        return layer\n",
    "\n",
    "    def forward(self, input):\n",
    "        semi_prediction = self.fusionBlock(input, self.num_channels)\n",
    "        semi_prediction = semi_prediction.split(2, dim=3)[1]\n",
    "        semi_prediction = torch.cat([semi_prediction, semi_prediction, semi_prediction], dim=3)\n",
    "\n",
    "        after_usim = self.USIM(semi_prediction, input, self.batch_size)\n",
    "\n",
    "        pred, conv = self.fusion2Block(after_usim, 3)\n",
    "\n",
    "        return pred, conv, semi_prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0895765-fba0-4fd9-b955-a6c0e43012e9",
   "metadata": {},
   "source": [
    "## Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63efb381-98c6-4d9b-a3b6-bd11c7fa8c41",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m imgHolder \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(\n\u001b[0;32m----> 2\u001b[0m     (args\u001b[39m.\u001b[39mbatch_size, args\u001b[39m.\u001b[39mheight, args\u001b[39m.\u001b[39mwidth, IMAGE_CHANNEL),\n\u001b[1;32m      3\u001b[0m     dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32,\n\u001b[1;32m      4\u001b[0m     device\u001b[39m=\u001b[39mdevice,\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      6\u001b[0m gndHolder \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(\n\u001b[1;32m      7\u001b[0m     (args\u001b[39m.\u001b[39mbatch_size, args\u001b[39m.\u001b[39mheight, args\u001b[39m.\u001b[39mwidth), dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mint32, device\u001b[39m=\u001b[39mdevice\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m boundary_weights \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros((), dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32, device\u001b[39m=\u001b[39mdevice)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from loss import *\n",
    "\n",
    "\n",
    "# 커맨드 라인 인자 파싱을 위한 ArgumentParser 생성\n",
    "parser = argparse.ArgumentParser(description=\"Enter valid args.\")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--image_path\",\n",
    "    \"--ip\",\n",
    "    default=\"./data/images/02_testset\",\n",
    "    metavar=\"IMAGE_PATH\",\n",
    "    help=\"로드하고 테스트할 이미지 경로.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--annotation_path\",\n",
    "    \"--ap\",\n",
    "    default=\"./data/annotations/02_testset\",\n",
    "    metavar=\"ANNOTATION_PATH\",\n",
    "    help=\"로드한 이미지에 대응하는 주석(Annotation) 경로\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--weight_path\", \"--wp\", default=None, help=\"사전 훈련된 가중치의 경로.\"\n",
    ")\n",
    "\n",
    "parser.add_argument(\"--height\", default=IMAGE_HEIGHT, help=\"이미지 높이\", type=int)\n",
    "parser.add_argument(\"--width\", default=IMAGE_WIDTH, help=\"이미지 너비\", type=int)\n",
    "parser.add_argument(\"--batch_size\", default=BATCH_SIZE, help=\"배치 크기\", type=int)\n",
    "parser.add_argument(\"--learning_rate\", default=1e-3, help=\"학습률\", type=float)\n",
    "parser.add_argument(\"--epoch\", default=100, help=\"에폭 수\", type=int)\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "# 이미지와 마스크를 저장할 리스트를 생성합니다.\n",
    "imgs = []\n",
    "gnds = []\n",
    "\n",
    "# 데이터로더를 통해 이미지와 마스크를 불러와서 리스트에 추가합니다.\n",
    "for batch in dataloader:\n",
    "    images, masks = batch\n",
    "    imgs.append(images)\n",
    "    gnds.append(masks)\n",
    "\n",
    "# 리스트에 있는 텐서들을 연결하여 하나의 텐서로 만듭니다.\n",
    "imgs = torch.cat(imgs, dim=0)\n",
    "gnds = torch.cat(gnds, dim=0)\n",
    "\n",
    "# 텐서를 넘파이 배열로 변환합니다.\n",
    "imgs = imgs.numpy()\n",
    "gnds = gnds.numpy()\n",
    "\n",
    "\n",
    "# 무작위 인덱스 생성하여 셔플\n",
    "indexes = np.asarray([i for i in range(0, len(imgs))])\n",
    "np.random.shuffle(indexes)\n",
    "\n",
    "# 디바이스 설정 (GPU 또는 CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 입력 이미지, 주석, 경계 가중치를 위한 텐서 플레이스홀더 생성\n",
    "imgHolder = torch.zeros(\n",
    "    (args.batch_size, args.height, args.width, IMAGE_CHANNEL),\n",
    "    dtype=torch.float32,\n",
    "    device=device,\n",
    ")\n",
    "gndHolder = torch.zeros(\n",
    "    (args.batch_size, args.height, args.width), dtype=torch.int32, device=device\n",
    ")\n",
    "boundary_weights = torch.zeros((), dtype=torch.float32, device=device)\n",
    "\n",
    "# B3SM 모델과 옵티마이저 생성\n",
    "model = B3SM(IMAGE_CHANNEL).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "average_loss_value = len(imgs)\n",
    "for epoch in range(1, args.epoch + 1):\n",
    "    loss_value = average_loss_value / (\n",
    "        float(args.batch_size) * (len(indexes) // args.batch_size + 1)\n",
    "    )\n",
    "    average_loss_value = 0\n",
    "    np.random.shuffle(indexes)\n",
    "\n",
    "    model.train()\n",
    "    for index in tqdm(\n",
    "        range(0, len(indexes), args.batch_size),\n",
    "        desc=\"epoch : (%03d/%03d) | loss : (%.4f)\" % (epoch, args.epoch, loss_value),\n",
    "    ):\n",
    "        batch_index = list(indexes[index : index + args.batch_size])\n",
    "        while len(batch_index) != args.batch_size:\n",
    "            batch_index.append(np.random.randint(0, len(indexes) - 1, 1)[0])\n",
    "\n",
    "        batch_index = np.asarray(batch_index)\n",
    "        batch_imgs, batch_gnds = (\n",
    "            imgs[indexes[batch_index]],\n",
    "            gnds[indexes[batch_index]],\n",
    "        )\n",
    "\n",
    "        # 배치 데이터를 텐서 플레이스홀더로 복사\n",
    "        imgHolder.copy_(torch.tensor(batch_imgs.transpose(0, 3, 1, 2)).to(device))\n",
    "        gndHolder.copy_(torch.tensor(batch_gnds).to(device))\n",
    "        boundary_weights.fill_(average_loss_value)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits, prediction, semi_logits = model(imgHolder) # 최종결과, 건물이 있을지에 대한 확률, \n",
    "        loss = (\n",
    "            IoU(logits, gndHolder)\n",
    "            + s1_loss(semi_logits, gndHolder, batch_size=args.batch_size)\n",
    "            * boundary_weights\n",
    "            * 10\n",
    "        )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        average_loss_value += loss.item() * args.batch_size\n",
    "\n",
    "        prd = (prediction[0] > 0).type(torch.uint8).cpu().numpy() * 255\n",
    "\n",
    "        lists = [batch_imgs[0], batch_gnds[0], prd]\n",
    "        [\n",
    "            cv2.imshow(\"이미지%d\" % window_index, image)\n",
    "            for window_index, image in enumerate(lists)\n",
    "        ]\n",
    "        [\n",
    "            cv2.moveWindow(\"이미지%d\" % window_index, 300 * window_index, 0)\n",
    "            for window_index, image in enumerate(lists)\n",
    "        ]\n",
    "        cv2.waitKey(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32eb51c-a3fe-4e11-a616-3a717ba16f7e",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12371c8b-0c78-47df-89ec-2d8b55c8ea94",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = SatelliteDataset(csv_file='./test.csv', transform=transform, infer=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "355b431c-ac8e-4c40-9046-4d53e4bab14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 3790/3790 [04:18<00:00, 14.65it/s]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    result = []\n",
    "    for images in tqdm(test_dataloader):\n",
    "        images = images.float().to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        masks = torch.sigmoid(outputs).cpu().numpy()\n",
    "        masks = np.squeeze(masks, axis=1)\n",
    "        masks = (masks > 0.35).astype(np.uint8) # Threshold = 0.35\n",
    "        \n",
    "        for i in range(len(images)):\n",
    "            mask_rle = rle_encode(masks[i])\n",
    "            if mask_rle == '': # 예측된 건물 픽셀이 아예 없는 경우 -1\n",
    "                result.append(-1)\n",
    "            else:\n",
    "                result.append(mask_rle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c2cbbb-04f1-4f9c-b4df-4b744dfce046",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6543d00-32b3-4f2d-a572-d0879fd0a497",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('./sample_submission.csv')\n",
    "submit['mask_rle'] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da10cb6f-0826-4755-a376-97b695ae8f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('./submit.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 1.14 (NGC 22.12/Python 3.8) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
